<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>DSAA2011 Gradient Descent</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<h1 id="gradiant-descent">Gradiant Descent</h1>
<h2 id="standard-gd">Standard GD</h2>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>𝐰</mi></msub><mi>C</mi><mo stretchy="false" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>𝐰</mi><mn>1</mn></msub></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>𝐰</mi><mn>2</mn></msub></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>C</mi></mrow><mrow><mi>∂</mi><msub><mi>𝐰</mi><mi>d</mi></msub></mrow></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_\mathbf{w}C(\mathbf{w})=\begin{bmatrix}
\frac{\partial C}{\partial \mathbf{w}_1} &amp; \frac{\partial C}{\partial \mathbf{w}_2} &amp; \dots &amp; \frac{\partial C}{\partial \mathbf{w}_d}
\end{bmatrix}
</annotation></semantics></math></p>
<p>PS: there is transport
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>⊤</mi><annotation encoding="application/x-tex">\top</annotation></semantics></math>)
in the slide, which does not fit the definition in AIAA2711 …</p>
<h3 id="persudo-code">Persudo Code</h3>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mtext mathvariant="normal">Algorithm GD</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">initialize </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>w</mi><mn>0</mn></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and learning rate </mtext><mspace width="0.333em"></mspace></mrow><mi>η</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mi>k</mi><mo>←</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">while </mtext><mspace width="0.333em"></mspace></mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> do</mtext></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><msub><mi>𝐰</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>←</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo>−</mo><mi>η</mi><msub><mi>∇</mi><mi>𝐰</mi></msub><mi>C</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>c</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> then</mtext></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">return </mtext><mspace width="0.333em"></mspace></mrow><msub><mi mathvariant="normal">w</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mtext mathvariant="normal">end if</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mtext mathvariant="normal">end while</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
&amp;\text{Algorithm GD} \\
&amp;~~~~\text{initialize }w_0\text{ and learning rate }\eta&gt;0 \\
&amp;~~~~k\leftarrow 0 \\
&amp;~~~~\text{while } true\text{ do} \\
&amp;~~~~~~~~\mathbf{w}_{k+1}\leftarrow\mathbf{w}_k-\eta\nabla_{\mathbf{w}}C(\mathbf{w}_k) \\
&amp;~~~~~~~~\text{if }coverage\text{ then} \\
&amp;~~~~~~~~~~~~\text{return }\mathrm{w}_{k+1} \\
&amp;~~~~~~~~\text{end if} \\
&amp;~~~~\text{end while}
\end{aligned}
</annotation></semantics></math></p>
<p><strong>impact</strong> of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>
:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>
is too small: Converge slowly</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>
is too large:
<ul>
<li>overshoot the loca</li>
<li>converge slowly to the minimum or may never converge</li>
</ul></li>
</ul>
<p>Posible convergence criteria:</p>
<ul>
<li><p>set maximum iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mi>max</mi><mo>&#8289;</mo></msub><annotation encoding="application/x-tex">k_{\max}</annotation></semantics></math></p></li>
<li><p>check the percentage or absolute change in objective function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>
below a threshold</p></li>
<li><p>check the percentage or absolute change in parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
below a threshold</p></li>
<li><p>设置最大迭代次数</p></li>
<li><p>当参数或者梯度小于某一个限定值，则停止。</p></li>
</ul>
<p>PS: GD can only find local minimum</p>
<ul>
<li>gradient
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">=0</annotation></semantics></math>
at local minimum, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>
will not change after reach local minimum.</li>
</ul>
<h2 id="variation-of-gd">Variation of GD</h2>
<h3 id="change-learning-rate">Change Learning Rate</h3>
<p><strong>Decreasing learning rate</strong>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>η</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>←</mo><msub><mi>η</mi><mi>k</mi></msub><mo>⋅</mo><mi>α</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> or </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>η</mi><mi>k</mi></msub><mo>−</mo><mi>α</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>𝐰</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>←</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo>−</mo><msub><mi>η</mi><mi>k</mi></msub><msub><mi>∇</mi><mi>𝐰</mi></msub><mi>C</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\eta_{k+1}&amp;\leftarrow\eta_k\cdot\alpha \text{ or } \eta_k -\alpha \\
\mathbf{w}_{k+1}&amp;\leftarrow\mathbf{w}_k-\eta_k\nabla_\mathbf{w}C(\mathbf{w}_k)
\end{aligned}
</annotation></semantics></math></p>
<p><strong>Adagrad</strong> (Adaptive Gradient Algorithm) :</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>G</mi><mi>k</mi></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><msubsup><mrow><mo stretchy="true" form="prefix">∥</mo><msub><mi>∇</mi><mi>𝐰</mi></msub><mi>C</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐰</mi><mi>s</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>𝐰</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>←</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo>−</mo><mfrac><mi>η</mi><msqrt><mrow><msub><mi>G</mi><mi>k</mi></msub><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac><msub><mi>∇</mi><mi>𝐰</mi></msub><mi>C</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
G_k&amp;=\sum_{s=0}^k \left\lVert\nabla_\mathbf{w}C(\mathbf{w}_s)\right\rVert^2_2 \\
\mathbf{w}_{k+1}&amp;\leftarrow \mathbf{w}_k-\frac{\eta}{\sqrt{G_k+\varepsilon}}\nabla_\mathbf{w}C(\mathbf{w}_k) \\
\end{aligned}
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>
is a small constant to prevent division by zero.</p>
<ul>
<li>Pros: adjust the learning rate for bases on the historical gradient
information can gives larger update to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathbf{w}_k</annotation></semantics></math>
with smaller gradient and smaller update to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathbf{w}_k</annotation></semantics></math>
with larger gradient. 使用历史的梯度参数可以使
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathbf{w}_k</annotation></semantics></math>
更新步幅更加均衡。</li>
<li>Cons: learning rate tends to shrink to quickly, leading to slow
convergence or getting stuck at suboptimal points (vanishing learning
rate) 可能会导致学习率减小过快，局限于某个次优值。</li>
</ul>
<h3 id="different-gradient">Different Gradient</h3>
<p><strong>Momentum-based GD</strong>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>𝐯</mi><mi>k</mi></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>←</mo><mi>β</mi><msub><mi>𝐯</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false" form="postfix">)</mo><msub><mi>∇</mi><mi>𝐰</mi></msub><mi>C</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>𝐰</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>←</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo>−</mo><mi>η</mi><msub><mi>𝐯</mi><mi>k</mi></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\mathbf{v}_k&amp;\leftarrow\beta\mathbf{v}_{k-1}+(1-\beta)\nabla_\mathbf{w}C(\mathbf{w}_k) \\
\mathbf{w}_{k+1}&amp;\leftarrow \mathbf{w}_k-\eta\mathbf{v}_k
\end{aligned}
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
is the momentum factor, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐯</mi><mn>0</mn></msub><mo>=</mo><msub><mi>∇</mi><mi>𝐰</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐰</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{v}_0=\nabla_\mathbf{w}(\mathbf{w}_0)</annotation></semantics></math>.</p>
<p>This algorithm can maintaining a moving average of past gradient. It
converges fast but can overshoot the minimum.</p>
<p><strong>Nesterov Acceleared Gradient (NAG)</strong> :</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>𝐯</mi><mi>k</mi></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>←</mo><mi>β</mi><msub><mi>𝐯</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>η</mi><msub><mi>∇</mi><mi>𝐰</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo>+</mo><mi>β</mi><msub><mi>𝐯</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>𝐰</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>←</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo>−</mo><msub><mi>𝐯</mi><mi>k</mi></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\mathbf{v}_k&amp;\leftarrow\beta \mathbf{v}_{k-1}+\eta\nabla_{\mathbf{w}}(\mathbf{w}_k+\beta\mathbf{v}_{k-1}) \\
\mathbf{w}_{k+1}&amp;\leftarrow\mathbf{w}_k-\mathbf{v}_k
\end{aligned}
</annotation></semantics></math></p>
<p>Concept: anticipate the direction the optimizer will go in the next
step. - Pros: faster convergence - Cons: slightly more complex than
momentum alone</p>
<h3 id="view-of-data-sheet">View of Data Sheet</h3>
<p><strong>Batch Gradient Descent(BGD)</strong> : Loss function is built
using the entire dataset at each iteration
使用整个数据集来计算损失函数的值。 e.g. 所有的loss取平均数</p>
<ul>
<li>Pros: converges smoothly to the minimum 收敛较为顺滑</li>
<li>Cons: computationally expensive, memory intensive, not ideal for
online learning (hard to update the model incrementally as new data
comes) 计算强度大，内存消耗大，难以支持动态添加新数据</li>
</ul>
<p><strong>Stochastic Gradient Descent(SGD)</strong> : use one
<strong>randomly chosen sample</strong> to build the loss function at
each iteration 每次迭代使用一个随机的样本计算损失函数</p>
<ul>
<li>Pros: fast updates, suitable for online learning
快速更新值，对动态添加数据友好</li>
<li>Cons: noisy update, longer convergence time
更新过程会出现噪声，不稳定，需要的收敛时间更长</li>
</ul>
<p><strong>Mini-Batch Gradient Descent(MBGD)</strong>: use some
<strong>randomly chosen samples</strong> to build the loss function at
each iteration 每次迭代选取一个小样本集合。</p>
<ul>
<li>Pros: between BGD and SGD</li>
<li>Cons: still has noise, hperparamters tuning (choice of batch
size)</li>
</ul>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mtext mathvariant="normal">Algorithm MBGD</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">initialize </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>w</mi><mn>0</mn></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and learning rate </mtext><mspace width="0.333em"></mspace></mrow><mi>η</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mi>k</mi><mo>←</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">divide </mtext><mspace width="0.333em"></mspace></mrow><mi>𝒟</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> into mini-batches</mtext></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">while </mtext><mspace width="0.333em"></mspace></mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> do</mtext></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">randomly choose one mini-batch </mtext><mspace width="0.333em"></mspace></mrow><mi>n</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">calculate </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>∇</mi><mi>𝐰</mi></msub><msub><mi>C</mi><mi>n</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><msub><mi>𝐰</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>←</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo>−</mo><mi>η</mi><msub><mi>∇</mi><mi>𝐰</mi></msub><msub><mi>C</mi><mi>n</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐰</mi><mi>k</mi></msub><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>c</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> then</mtext></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mrow><mtext mathvariant="normal">return </mtext><mspace width="0.333em"></mspace></mrow><msub><mi mathvariant="normal">w</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mtext mathvariant="normal">end if</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mtext mathvariant="normal">end while</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
&amp;\text{Algorithm MBGD} \\
&amp;~~~~\text{initialize }w_0\text{ and learning rate }\eta&gt;0 \\
&amp;~~~~k\leftarrow 0 \\
&amp;~~~~\text{divide }\mathcal{D}\text{ into mini-batches} \\
&amp;~~~~\text{while } true\text{ do} \\
&amp;~~~~~~~~\text{randomly choose one mini-batch } n \\
&amp;~~~~~~~~\text{calculate }\nabla_\mathbf{w}C_n(\mathbf{w}_k) \\
&amp;~~~~~~~~\mathbf{w}_{k+1}\leftarrow\mathbf{w}_k-\eta\nabla_{\mathbf{w}}C_n(\mathbf{w}_k) \\
&amp;~~~~~~~~\text{if }coverage\text{ then} \\
&amp;~~~~~~~~~~~~\text{return }\mathrm{w}_{k+1} \\
&amp;~~~~~~~~\text{end if} \\
&amp;~~~~\text{end while}
\end{aligned}
</annotation></semantics></math></p>
</body>
</html>
